\documentclass[12pt, letterpaper]{article}
\usepackage[utf8]{inputenc}

\title{First document}
\author{Hubert Farnsworth \thanks{funded by the Overleaf team}}
\date{February 2014}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphics}
\usepackage{physics}

\begin{document}
\tableofcontents
\newpage
\begin{center}
\section{INTEGER PROGRAMMING}
\subsection{ Branch and Bound Algorithm }
\end{center}


\section{QUEUEING THEORY}
\subsection{Introduction}
To study the queue we need to understand the queue characteristics.
QT is about understanding the Queue Characteristics.
Queue Characteristics can be modelled using:

\begin{itemize}
    \item Exponential Distribution.
    \item Poisson Distribution.
\end{itemize}

\subsection{Elements of Queueing Theory}
 Principal Actors
\begin{itemize}
    \item Customer
    \item Server (A facility which gives service the customer)
\end{itemize}

Queue:
\begin{itemize}
    \item Finite.
    \item Infinite.
\end{itemize}

Queue Discipline:
\begin{itemize}
    \item FCFS (First Come First Serve)
    \item LCFS (Last Come First Serve)
    \item SIRO
    \item Order of Priority.
\end{itemize}

Queue Behaviour:
\begin{itemize}
    \item Jockeying.
    \item Balking.
    \item Reneging.
\end{itemize}

Waiting Line Process (Queueing Process)
\begin{itemize}
    \item Customers
    \item Service.
\end{itemize}

Queueing Process can be of Single Server or Multiple Server.
\begin{itemize}
    \item Single Server $\rightarrow$ only 1 service station for the entire queue. (Movie ticket booking, food market).
    \item Multiple Server $\rightarrow$ Gas station with multiple serving station.
\end{itemize}

The process of waiting line is characterized by the rate at which the customer arrives. 

\begin{itemize}
    \item Arrival Rate specifies the average number of customers per time period.
    \item The service rate specifies the average number of customers served during a time period.
\end{itemize}

\emph{What we need to do?} \\
Our task is to determine the probability of ‘n’arrivals being observed during a time interval of length ‘t’.

\emph{The probability is based on the following pre-assumptions:}
\begin{itemize}
    \item The Probability that an arrival is observed during a small time interval is proportional to the length of the interval.

    \item The probability of two or more arrivals in such a small interval is zero.

    \item The no. of arrivals in any time interval is independent of the number in non-overlapping time interval.
\end{itemize}

\begin{center}
    P( ‘n’ customers during period ‘t’) = the probability that ‘n’ arrivals will be observed in a time interval of length ‘t’.
\end{center}

\begin{equation}
    P(n arrivals in time t) = \lambda e^{-\lambda t}
\end{equation}

\subsection{Markov modelling (M/M/1 model) ( $\alpha$ / FIFO)}
\subsubsection{Assumptions of M/M/1 Model}
Assumptions for the M/M/1 Modelling:
\begin{itemize}
    \item Arrivals follow poisson’s distribution, with lambda.
    \item Service time follows exponential distribution with mu, Average service rate.
    \item Arrivals are infinite populations ($\alpha = \infty$)
    \item Customers are served in FIFO.
    \item There is only a single server.
\end{itemize}
 
\subsubsection{The M/M/1 Model}
For the system to work, No. Of arrivals $<$ Avg. Service rate.

$$\rho = \frac{\lambda}{\mu} $$ 
$ \rho \leftarrow $ Probability of time.

Probability that system is idle = Probability that there are no customers in the system.

$$P_0 = 1 - \rho$$.

Probablity of having exactly one customer in the system.
$$ P_n = \rho^n P_0  = $$
The expected number of customers in the \emph{System}:
$$ L_s =  \sum_{n=1}^{\alpha (\infty)} n P_n 
= \frac{\rho}{(1-\rho)} = \frac{\lambda}{\mu-\lambda} $$
The expected number pf customers in the \emph{Queue}:
$$ L_q = \sum_{n=1}^{\alpha} (n-1)P_n  = \frac{\rho^2}{(1 - \rho)} = \frac{\lambda^2}{\mu(\mu - \lambda)}$$
Average Waiting time in the \emph{Queue}:

$W_q =$ (Avg. Time b/w arrivals) * $L_q$  $$ W_q = \frac{1}{\lambda}Lq $$

\subsection{Kendall Notation}
$a =$ Arrival distribution \\
$b =$ Service time (has exponential distribution) \\
$c =$ no. Of service channels (Servers) \\
$d =$ Maximum number of customers allowed in system \\
$e =$ queue discipline (FIFO / FCFS) \\

[ (a/b/c) = (d/e)]
\section{NONLINEAR PROGRAMMING} 
\subsection{Introduction}

\subsection{Search Based Methods}
\subsubsection{Fibonacci Method}
Fibonacci method can be used to find the minimum of a function of one variable even if the function is not continuous.

It is also similar to dichotomous method but the two are not the same i.e. is applicable also for  non continuous functions also.

\begin{center}
    \textbf{Limitations of the Fibonacci Method: }
\end{center}
\begin{itemize}
    \item The initial interval of uncertainty, in which the optimum lies, has to be known.
    \item The function to be optimized has to be \emph{unimodal} (in an interval only 1 minimum can lie) in the initial interval of uncertainty.
    \item The exact optimum cannot be located in this method. Only an interval known as the final interval of uncertainty can be made as small as desired by using more computation.
    \item The number of function evaluations to be used in the search or the resolution required has to be specified beforehand.
\end{itemize}

\begin{equation}F_0 = F_1 = 1\end{equation}
\begin{equation}F_n = F_{n-1} + F_{n-2} \hspace{0.25cm} n=2,3,4...\end{equation}
\begin{equation}L_0 = [a, b] \end{equation}
\begin{equation}L_2^{*} = \frac{F_{n-2}}{F_{n}}L_0\end{equation}
\begin{equation}x_1 = a + L_2^{*} = a + \frac{F_{n-2}}{F_n}L_0 \end{equation}
\begin{equation}x_2 = b - L_2^* = a + \frac{F_{n-2}}{F_n}L_0 \end{equation}
\begin{equation}L_2 = L_0 - L_2^* = L_0 - \frac{F_{n-2}}{F_n}L_0 = \frac{F_{n-1}}{F_n}L_0\end{equation}
\begin{equation}L_2^* = \frac{F_{n-2}}{F_n}L_0 = \frac{F_{n-2}}{F_{n-1}}L_2\end{equation}
\begin{equation}L_3^*  = \frac{F_{n-3}}{F_n}L_0 = \frac{F_{n-3}}{F_{n-1}}L_2\end{equation}
\begin{equation}
L_3 = L_2 - L_3^* = L_2 - \frac{F_{n-3}}{F_{n-1}}L_2 = \frac{F_{n-2}}{F_{n-1}}L_2    
\end{equation}

After $j^{th}$ iteration:
(Start j from 3)

\begin{equation}L_j = \frac{F_{n-j}}{F_{n-(j-2)}}L_{j-1}\end{equation}
\begin{equation}L_j = \frac{F_{n-(j-1)}}{F_n}L_0\end{equation}
$$ Reduction \hspace{0.1cm} Ratio  =\frac{L_n}{L_0}$$


$$ n \hspace{1cm} F_n \hspace{1cm} \frac{L_n}{L_0}  $$
$$ 0 \hspace{1cm} 1	  \hspace{1cm}		1 $$
$$ 1 \hspace{1cm} 1	  \hspace{1cm}		1 $$
$$ 2 \hspace{1cm} 2   \hspace{1cm}		0.5 $$
$$ 3 \hspace{1cm} 3   \hspace{1cm}   	0.333 $$
$$ 4 \hspace{1cm} 4   \hspace{1cm}		0.2 $$
$$ 5 \hspace{1cm} 8   \hspace{1cm}		0.125 $$
$$ 6 \hspace{1cm} 13  \hspace{1cm}		0.07692 $$
$$ 7 \hspace{1cm} 21  \hspace{1cm}		0.04762 $$
$$ 8 \hspace{1cm} 34  \hspace{1cm}		0.02941 $$
$$ 9 \hspace{1cm} 55  \hspace{1cm}		0.01818 $$
$$ 10 \hspace{1cm}89  \hspace{1cm}		0.01124 $$

\subsubsection{Golden Search Method}
Golden Search Method is based on the \textbf{Golden Ratio}.
Ex. 1>  A line segment is divided into 2 unequal parts:\\
$\leftarrow$-------------------------------------$\rightarrow$ \\
$\leftarrow$--------- a ---------$\rightarrow$ $\leftarrow$---b---$\rightarrow$

$$\frac{(a+b)}{a} = \frac{a}{b} ( =r )$$  \begin{center}For what value of  ‘a’ can this happen?\end{center}

$$ \frac{b}{(a+b)} = a^2$$

\begin{center}
    for b =1
\end{center}
$$ a^2 -a -1 = 0 $$
$$ a = \frac{1+\sqrt{5}}{2} $$

$$  \frac{a}{b} = \frac{1 + \sqrt{5}}{2} = 1.6180339$$

Now from fibonacci method, for large $N (N \rightarrow \infty)$

$$ L_2 = \lim_{N \rightarrow \infty} F_{n-2} $$

At Kth  iteration:
$$ L_k = \lim_{N \rightarrow \infty} (\frac{F_{n-1}}{F_n})^{k-1} L_0 $$
Now, from the relation:
$$ F_n = F_{n-1} + F_{n-2} $$
$$ \frac{F_n}{F_{n-1}} = 1 + \frac{F_{n-2}}{F_{n-1}} $$
\begin{equation}\frac{F_n}{F_{n-1}} = 1 + \frac{F_{n-1}}{F_n} \end{equation}

Let’s define
\begin{equation} r = \lim_{N \rightarrow \infty} \frac{F_n}{F_{n-1}}  \end{equation}
\begin{equation} r = 1 + \frac{1}{r} \end{equation}
\begin{equation} r^2 - r -1 = 0\end{equation}
\begin{equation}
  r = 1.618034  
\end{equation}

using (3) and (4) in (1):
$$ L_k = (\frac{1}{r})^{k-1} L_0
      = (\frac{1}{1.618034})^{k-1} L_0 $$

$$ L_2^* = \lim_{N \rightarrow \infty} (\frac{F_{n-2}}{F_n}) L_0
        = \lim_{N \rightarrow \infty} (\frac{F_{n-1}}{F_n})^2 L_0
        = (\frac{1}{r})^2 L_0 $$


\subsection{Constrained Multivariate Optimization}
\subsubsection{Lagrange Multiplier Method}
The \emph{Langrangian multiplier method} is very useful for constraint multivariate optimisation.\\
\textbf{Optimize function $f(x,y,z)$ subject to a constraint $g(x,y,z)=c$.}

\textbf{Lagrangian Function}

$$L(x,y,z,\lambda_1,\lambda_2) = f(x,y,z) - \sum_{i=1}^{m} \lambda_i g_i(x,y,z)$$

\subsubsection{Convex \& Concave Function}
\subsubsection{Nonlinear Multivariate Optimization with Equality Constraints}
\textit{}
\begin{itemize}
    \item $2x - 2\lambda_1 = 0 		=> x = \lambda_1$
    \item $2 + \lambda_1 - \lambda_2  = 0		=> \lambda_2 = \lambda_1 + 2$
    \item $-2z - \lambda_2 = 0		=> z = -\frac{\lambda_2}{2}$
    \item $2x - y = 0			=> 2x + z = 0$
    \item $y + z = 0$
\end{itemize}
Solving the above equations:
\begin{itemize}
    \item $\lambda_1 = \frac{2}{3}$ 
    \item $\lambda_2 = \frac{8}{3}$ 
    \item $x = \frac{2}{3}$ 
    \item $y = \frac{4}{3}$ 
    \item $z = -\frac{4}{3}$
\end{itemize}
The maximum point is $(\frac{2}{3},\frac{4}{3},-\frac{4}{3})$
and the maximum value = $f(\frac{2}{3},\frac{4}{3},-\frac{4}{3}) = \frac{4}{9} + \frac{8}{3} - \frac{16}{9} = \frac{4}{3}$

\textit{Ex : Find optimal value of: $$f(x,y,z) = x^2 + y^2 + z^2$$ 
		w.r.t the constraints:
\begin{itemize}
    \item $x + 2y + 3z = 6$
    \item $x + 3y + 9z = 9$
\end{itemize}
}

\subsubsection{Nonlinear Multivariate Optimization with Inequality Constraints}

Max/Min $f(x_1,x_2,....,x_n)$ \\
S.T. \\
$g_1(x_1,x_2,...,x_n) <= b_1$ \\
$g_2(x_1,x_2,...,x_n) <= b_2$ \\
$g_m(x_1,x_2,...,x_n) <= b_m$ \\
$x_1,x_2,....,x_n >= 0$

\subsubsection{The Kuhn-Tucker Conditions}

\subsection{Quadratic Programming}
Quadratic Programming is the problem of optimizing a quadratic objective function.
\textbf{Quadratic Programming is a type of Nonlinear Unconstrained Optimization.}

\emph{Important points regarding Quadratic Programming:}
\begin{itemize}
    \item The objective function can be bilinear or a 2nd order polynomial term.
    \item The constraints are linear and can be inequalities and equalities.
\end{itemize}

The \textbf{Hessian Matrix} is used to detect the \textbf{nature of stationary points} in a \textbf{multivariable function}.

\begin{equation}
H =
\begin{bmatrix}
\pdv[2]{f}{x_1} & \pdv{f}{x_1}{x_2} & ... & \pdv{f}{x_1}{x_n}\\
\pdv{f}{x_2}{x_1} & \pdv[2]{f}{x_2} & ... & \pdv{f}{x_2}{x_n}\\
\pdv{f}{x_3}{x_1} & \pdv{f}{x_3}{x_2} & ... & \pdv{f}{x_3}{x_n}\\
. & . & ... & .\\
. & . & ... & .\\
. & . & ... & .\\
\pdv{f}{x_n}{x_1} & \pdv{f}{x_n}{x_2} & ... & \pdv[2]{f}{x_n}\\
\end{bmatrix}
\end{equation}

\emph{Definiteness of Matrices:} \\
\textbf{Positive Definiteness:} \\ 
When all the \textit{principal minors} of a matrix are \textit{positive}, then the matrix is said to be \textbf{positive definite}. \\
\textbf{Positive Semi-definiteness:}  \\
When all the \textit{principal minors} of a matrix are \textit{non-negative}, then the matrix is said to be \textbf{positive semidefinite}. \\
\textbf{Negative Definiteness: }  \\
When all the \textit{principal minors} of a matrix are \textit{negative}, then the matrix is said to be \textbf{negative definite}. \\
\textbf{Negative Semi-definiteness:}  \\
When all the \textit{principal minors} of a matrix are \textit{non-positive}, then the matrix is said to be \textbf{negative semi-definite}.\\
\textbf{Indefiniteness:}  \\
When the \textit{principal minors} of a matrix are \textit{neither all positive nor all negative}, then the matrix is said to be \textbf{indefinite}.\\

A matrix A is said to be positive semidefinite, if all the eigenvalues of 

\textbf{Taylor Series Expansion of Multivariable Functions: }
\begin{equation}f(x+x^*) = f(x^*) + (\nabla f(x^*))^T x + \frac{1}{2}x^T H(x^*) x\end{equation}
\begin{equation} f(x+x^*) = q + p^T x + \frac{1}{2}x^T H x\end{equation}

where,
$$x = \begin{bmatrix}x_1 \\ x_2 \\ x_3 \\.\\.\\ x_n \end{bmatrix}
  x^* = \begin{bmatrix}x_1^* \\ x_2^* \\ x_3^* \\.\\.\\ x_n^* \end{bmatrix}$$
  
 comparing equations (20) \& (21): \\
 $$q = f(x^*)$$
 $$p = \nabla f(x^*)$$
 $$H = H(x^*)$$
\subsubsection{Unconstrained Optimization}
\emph{Ex - 1 \\
Find the minima of the following quadratic objective function: \\
    $f(x_1,x_2) = 3x_1^2 + 4x_1x_2 + 3x_2^2 - 50x_1 - 50x_2$ } \\
    
\textbf{Soln: }
Let $x^*$ be the stationary point.
$\therefore$  $\nabla f(x^*) = \overrightarrow 0 $
\begin{equation}
    \nabla f(x*) = \begin{bmatrix}6x_1^* + 4x_2^* - 50 \\ 4x_1^* + 6x_2^* - 50\end{bmatrix} = \begin{bmatrix}
    0 \\ 0
    \end{bmatrix}
\end{equation}

\begin{equation}
  x_1^* = 5 , x_2^* = 5
\end{equation}

\begin{equation}
    H(x^*) = \begin{bmatrix}
\pdv[2]{f}{x_1} & \pdv{f}{x_1}{x_2} \\
\pdv{f}{x_2}{x_1} & \pdv[2]{f}{x_2} 
\end{bmatrix} = \begin{bmatrix}
6 & 4 \\
4 & 6
\end{bmatrix}
\end{equation}

$\therefore$ \hspace{0.2cm} $1^{st}$ order principal minor of H = 6

$\therefore$ \hspace{0.2cm} $2^{nd}$ order principal minor of H = 36 - 16 = 20

Since, both the principal minors of the matrix are positive, so H is positive definite and $f(x_1,x_2)$ has one minima at $x^*$.

Functional Value at the minimum:
$$ f(5,5) = -250 $$
\subsubsection{Quadratic Programming with equality constraints (Bordered Hessian Matrix)}

A Quadratic Programming problem with equality constraints is of the following form:

Min/Max $f(x_1,x_2,x_3,...,x_n)$

Subject To: \\
$ g_1(x_1,x_2,x_3,...,x_n) = b_1$ \\
$ g_2(x_1,x_2,x_3,...,x_n) = b_2$ \\
$ g_3(x_1,x_2,x_3,...,x_n) = b_3$ \\
. \\
. \\
$ g_m(x_1,x_2,x_3,...,x_n) = b_m$ \\
$x_1, x_2, x_3, ... x_n \geq 0 $

Then, the Laplacian:
    $$L(x_1,x_2,...,x_n,\lambda_1,\lambda_2, ... \lambda_m) = f(\overrightarrow x) + \sum_{i=1}^m \lambda_i [b_i - g_i(\overrightarrow x)]$$
    
    Solving $\nabla L = \overrightarrow 0$
    We get, stationary point $x^*$ \& $\lambda_i$ values.
    \\
    \\
    \\
\textbf{Bordered Hessian Matrix}
\begin{equation}
    \Tilde{H} = \begin{bmatrix}
    [0]_{m*m} && [\nabla g_i^T]_{m*n} \\ [\nabla g_i]_{n*m} && [H]_{n*n}
    \end{bmatrix}_{(m+n)*(m+n)}
\end{equation}

Where, 

$ n = $Number of independent variables.

$ m = $Number of constraints.

At the critical point $x*$
\begin{itemize}
    \item If the $\begin{vmatrix}H\end{vmatrix} = M_{m+n}$ has sign $(-1)^n$ and the sign of the determinant of the last (m+n) leading principal minors $M_{2m+1},...,M_{m+n}$ alternate in sign, then H is positive definite and the critical point $x^*$ is a minimum point.
    
    \item If the determinant of all (m+n) leading principal minors $M_{2m+1},..., M_{m+n}$ have the same sign $(-1)^m$, then H is positive definite and $x^*$ is a minimum point. 
\end{itemize}

\emph{Ex-1 \\
Find the extreme points of $f(x,y,z) = x^2 + y^2 + z^2$}

Subject To:
$$ 3x + y + z = 5 $$
$$ x + y + z = 1 $$
$$ x,y,z \geq 0 $$


\textbf{Soln.}

The Lagrangian Function:
\begin{equation}
    L(x,y,z,\lambda_1,\lambda_2) = x^2 + y^2 + z^2 + \lambda_1(5-3x-y-z) + \lambda_2(1-x-y-z)
\end{equation}
\begin{equation}
    g_1(x,y,z) = 3x + y + z
\end{equation}
\begin{equation}
    g_2(x,y,z) = x + y + z
\end{equation}

We, Solve the following system of equations to find the extreme points:
\begin{equation}
    \nabla L = \overrightarrow 0
\end{equation}
\begin{equation}
    \pdv{L}{x} = 0 \implies 2x-3\lambda_1-\lambda_2 = 0
\end{equation}
\begin{equation}
    \pdv{L}{y} = 0 \implies 2y-\lambda_1-\lambda_2 = 0
\end{equation}
\begin{equation}
    \pdv{L}{z} = 0 \implies 2z-\lambda_1-\lambda_2 = 0
\end{equation}
\begin{equation}
    \pdv{L}{\lambda_1} = 0 \implies 3x+y+z = 0
\end{equation}
\begin{equation}
    \pdv{L}{\lambda_2} = 0 \implies x+y+z = 0
\end{equation}

Solving (30) - (34) , we get,

$x=2$, $y=-\frac{1}{2}$, $z=-\frac{1}{2}$, $\lambda_1 = \frac{5}{2}$ \& $\lambda_2=-\frac{7}{2}$\\


$\therefore$ The\hspace{0.1cm} Critical\hspace{0.1cm} Point ($x^*$) = $(2,-\frac{1}{2},-\frac{1}{2})$


The Hessian Matrix: 
\begin{equation}
    H = \begin{bmatrix}
    \pdv[2]{f}{x} && \pdv{f}{x}{y} && \pdv{f}{x}{z} \\
    \pdv{f}{y}{x} && \pdv[2]{f}{y} && \pdv{f}{y}{z} \\
    \pdv{f}{z}{x} && \pdv{f}{z}{y} && \pdv[2]{f}{z} 
    \end{bmatrix} = \begin{bmatrix}
    2 & 0 & 0 \\ 0 & 2 & 0 \\ 0 & 0 & 2
    \end{bmatrix}
\end{equation}

The \textbf{Bordered Hessian Matrix}:
\begin{equation}
    \Tilde{H} = 
    \begin{bmatrix}
    0 & 0 & 3 & 1 & 1 \\ 0 & 0 & 1 & 1 & 1 \\ 3 & 1 & 2 & 0 & 0 \\ 1 & 1 & 0 & 2 & 0 \\ 1 & 1 & 0 & 0 & 2
    \end{bmatrix}
\end{equation}
\end{document}

